<!DOCTYPE html>
<html lang="en"><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset="utf-8"><meta name="generator" content="Hugo 0.133.0"><meta name="viewport" content="width=device-width,initial-scale=1,viewport-fit=cover">
<meta name="color-scheme" content="light dark">
<meta name="supported-color-schemes" content="light dark"><title>2023.8.8&nbsp;&ndash;&nbsp;santiweide&#39;s Blog</title><link rel="stylesheet" href="/css/core.min.003a667cf21024ef30f2985fb57c9bd6f6873cf899ee14e9302e68e9f04b681df9440cec4ebdaa101dca533a262c242f.css" integrity="sha384-ADpmfPIQJO8w8phftXyb1vaHPPiZ7hTpMC5o6fBLaB35RAzsTr2qEB3KUzomLCQv"><meta name="twitter:card" content="summary" />
<meta name="twitter:title" content="2023.8.8" /><body><section id="header">
    <div class="header wrap"><span class="header left-side"><a class="site home" href="/"><span class="site name">santiweide's Blog</span></a></span>
        <span class="header right-side"><div class="nav wrap"><nav class="nav"><a class="nav item" href="/tags/">Tags</a><a class="nav item" href="/about">About</a><a class="nav item" href="/friend">友链</a><a class="nav item" href="https://gohugo%2eio/"target="_blank" rel="noopener noreferrer">Hugo</a></nav></div></span></div><div class="site slogan"><span class="title">露の世は露の世ながらさりながら</span></div></section><section id="content"><div class="article-container"><section class="article header">
    <h1 class="article title">2023.8.8</h1><p class="article date">Wednesday, August 9, 2023</p></section><article class="article markdown-body"><h1 id="202388">2023.8.8</h1>
<p>工作取得突破性进展的一天！意识到早点和策略沟通真的可以解决好多模型结构问题哦，比如这次的query &amp; key的params shape配置，以及y=ax+b的b shape一定是a的第一维这件事。</p>
<p>晚上成功打球！激动开心，打工的放风时间～！医生姐姐技巧max，小鹿连带一个小时的球，据说都没有把球发给她的后场过（大佬的自信）</p>
<p>最近一个比较焦虑的事情，就是同事离职了。因为自己不知道是不是qualified去像同事那样离职。虽然每天在一起工作，模型相关的知识自己的积累还是不够。于是很担心同事离职……这种焦虑或许可以通过完善自身来缓解…..</p>
<p>我歌很鼓励我，发的cr还给我+2…萌哥也是，我看代码遇到的问题萌哥会帮我认真解答、认真听我说。感受到了鼓励，感动！</p>
<p>最近开发ft也有一些感悟，比如：</p>
<ol>
<li>weight中param的shape可以说是模型计算的主线角色，也是链接训练和预估框架的桥梁。但是也是因为weight出现在好多数据结构里面，导致weight相关的改动需要适配的文件很多</li>
</ol>
<h1 id="gemm">gemm</h1>
<p><a href="https://www.cnblogs.com/cuancuancuanhao/p/7763256.html"target="_blank" rel="noopener noreferrer">https://www.cnblogs.com/cuancuancuanhao/p/7763256.html</a></p>
<p><strong>Gemm shape</strong></p>
<p>local_hidden_units_ = local_head_num_ * size_per_head_</p>
<p>local_head_num_ = head_num / tensor_para.world_size_</p>
<p>hidden_units_ = head_num * size_per_head_</p>
<p>DecoderSelfAttention</p>
<p>Gemm:</p>
<p>A [3 * local_head_num_ * size_per_head_, d_model_]</p>
<p>B [d_model_, batch_size]</p>
<p>C [3 * local_head_num_ * size_per_head_, batch_size]</p>
<p>其中A为qkv weight, B 为input query</p>
<p>GptContextLayer</p>
<p>Gemm:</p>
<p>A [3 * local_head_num_ * size_per_head_, hidden_units_]</p>
<p>B [hidden_units_, token_num] = [hidden_units_, batch_size * seq_len]</p>
<p>其中A为qkv weight, B 为input query</p>
<p><strong>converter逻辑</strong></p>
<p>i循环mainloop = 训练和推理中较小的那个tensor_parallel_size</p>
<p>j循环pipeline_parallel_size</p>
<p><strong>FastTransformer中和weight shape变化相关的逻辑</strong></p>
<ol>
<li>
<p>ParallelGptDecoderLayerWeight</p>
<p>这个类是ContextDecoder和Decoder共享的类，只在加载GptWeight的时候load一次，后面ContextDecoder和generate阶段的Decoder共享。</p>
<p>shape变化：</p>
<p>相关接口：</p>
<p>copyFrom</p>
<p>loadModel</p>
<p>mallocWeights</p>
<p>setWeightPtr</p>
<p><strong>copyFrom函数</strong></p>
<p>这个函数的出现主要是为了深拷贝。Weight类的=operator reload和对象拷贝构造函数都是先mallocWeight from ckpt file，然后调用copyFrom赋值，最后setWeightPtr把临时存储的weight指针赋值给对应的模型结构。</p>
</li>
<li>
<p>激活函数的weight</p>
<p>在ft中，激活函数和ffn layer写到了一起。ffn的权重是通过FfnWeight存储的。</p>
</li>
</ol>
<p><strong>convert脚本</strong></p>
<p>llama2和gpt在converter的差别包括：</p>
<blockquote>
<p>1 没有positional embedding，使用了RoPE</p>
</blockquote>
<blockquote>
<p>2 query、key_value的weights&amp;bias是分开两组文件存储的，因为使用了GQA(Group MultiQuery Attention), shape不一样。</p>
</blockquote>
<p><strong>运行 &amp; 预运行</strong></p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># enter the docker</span>
</span></span><span class="line"><span class="cl">nvidia-docker run -it --shm-size 200G --name hwj_test  --network host -v /home/disk1/huwanjing/trt2022_src:/target   registry.baidubce.com/paddlepaddle/paddle:2.4.2-gpu-cuda11.2-cudnn8.2-trt8.0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># multigpu</span>
</span></span><span class="line"><span class="cl">nvidia-docker run -it --shm-size 200G --name hwj_multigpu_test  --network host -v /home/disk1/huwanjing/trt2022_src:/target   nvcr.io/nvidia/pytorch:23.04-py3
</span></span><span class="line"><span class="cl">cmake -DSM<span class="o">=</span><span class="m">80</span> -DCMAKE_BUILD_TYPE<span class="o">=</span>Release -DBUILD_MULTI_GPU<span class="o">=</span>ON ..
</span></span><span class="line"><span class="cl"><span class="c1"># set network</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">https_proxy</span><span class="o">=</span>http://172.19.56.199:3128
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">http_proxy</span><span class="o">=</span>http://172.19.56.199:3128
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># install for pretrainerd tokenizer</span>
</span></span><span class="line"><span class="cl">pip install sentencepiece
</span></span><span class="line"><span class="cl">pip install transformers
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># install torch, nvidia&#39;s apex, regex</span>
</span></span><span class="line"><span class="cl">pip install <span class="nv">torch</span><span class="o">==</span>1.13.0+cpu --extra-index-url https://download.pytorch.org/whl/cpu
</span></span><span class="line"><span class="cl">pip install regex
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /target/python_home/apex
</span></span><span class="line"><span class="cl">python setup.py install
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /target/hwj_test
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">mkdir hwj_test
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> hwj_test
</span></span><span class="line"><span class="cl">git clone -b expt-llama-dev ssh://huwanjing@icode.baidu.com:8235/baidu/model-service/swifttransformer baidu/model-service/swifttransformer <span class="o">&amp;&amp;</span> git config -f baidu/model-service/swifttransformer/.git/config user.name huwanjing <span class="o">&amp;&amp;</span> git config -f baidu/model-service/swifttransformer/.git/config user.email huwanjing@baidu.com <span class="o">&amp;&amp;</span> curl -s https://icafe-cli.now.baidu-int.com/git-hooks/install.sh <span class="p">|</span> bash -s -- -g baidu/model-service/swifttransformer/.git
</span></span><span class="line"><span class="cl">git clone https://github.com/NVIDIA/Megatron-LM.git
</span></span><span class="line"><span class="cl"><span class="c1"># add megatron (from github) into PYTHONPATH</span>
</span></span><span class="line"><span class="cl"><span class="nb">unset</span> PYTHONPATH
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">PYTHONPATH</span><span class="o">=</span>/root/paddlejob/workspace/env_run/hwj_trans/Megatron-LM:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">rm -rf /root/paddlejob/workspace/env_run/gqa_model/ft-model.8
</span></span><span class="line"><span class="cl">python ./examples/pytorch/gpt/utils/megatron_llama2_gqa_ckpt_convert.py <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -head_num <span class="m">40</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --kv-head-num <span class="m">8</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -i /root/paddlejob/workspace/env_run/gqa_model/iter_0204000 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -o /root/paddlejob/workspace/env_run/gqa_model/ft-model.8 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -t_g <span class="m">8</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -i_g <span class="m">8</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --vocab-path /root/paddlejob/workspace/env_run/du_tokenizer_v2.model <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -processes<span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#海波的模型：</span>
</span></span><span class="line"><span class="cl"><span class="c1">#新增inter size配置，因为</span>
</span></span><span class="line"><span class="cl"><span class="c1">#13632 = 2/3*4*5120 遵循了llama实现， intermediate_size = 2 / 3 * 4* hidden_size</span>
</span></span><span class="line"><span class="cl"><span class="c1"># 对齐到256的整数倍就是13632</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">unset</span> PYTHONPATH
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">PYTHONPATH</span><span class="o">=</span>/target/hwj_test/megatron.source/:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">rm -rf /target/hwj_test/llama2/mha-model/ft-model/2-gpu
</span></span><span class="line"><span class="cl">python ./examples/pytorch/gpt/utils/megatron_llama2_mha_ckpt_convert.py <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -head_num <span class="m">40</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -i /target/hwj_test/llama2/mha-model/iter_0000246 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -o /target/hwj_test/llama2/mha-model/ft-model/ <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --trained-tensor-parallel-size <span class="m">1</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --infer-gpu-num <span class="m">2</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --vocab-path /target/hwj_test/llama2/pretrain_tokenizer/mha.tokenizer.model
</span></span><span class="line"><span class="cl">        --processes<span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">unset</span> PYTHONPATH
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">PYTHONPATH</span><span class="o">=</span>/target/hwj_test/megatron.source:<span class="si">${</span><span class="nv">PYTHONPATH</span><span class="si">}</span>
</span></span><span class="line"><span class="cl">rm -rf /root/paddlejob/workspace/env_run/output/huwanjing/mha_model/mha-ckpt1/
</span></span><span class="line"><span class="cl">python ./examples/pytorch/gpt/utils/megatron_llama2_mha_ckpt_convert.py <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -head_num <span class="m">40</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -i /root/paddlejob/workspace/env_run/output/huwanjing/mha_model/iter_0000246 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        -o /root/paddlejob/workspace/env_run/output/huwanjing/mha_model/mha-ckpt1 <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --trained-tensor-parallel-size <span class="m">1</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --infer-gpu-num <span class="m">2</span> <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --vocab-path /root/paddlejob/workspace/env_run/output/huwanjing/mha_model/vocab.json <span class="se">\
</span></span></span><span class="line"><span class="cl"><span class="se"></span>        --processes<span class="o">=</span><span class="m">1</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">mpirun -n <span class="m">2</span> --allow-run-as-root ./bin/multi_gpu_gpt_example
</span></span></code></pre></div><p>报错记录</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">Traceback <span class="o">(</span>most recent call last<span class="o">)</span>:
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;./examples/pytorch/gpt/utils/megatron_ckpt_convert.py&#34;</span>, line 652, in &lt;module&gt;
</span></span><span class="line"><span class="cl">    main<span class="o">()</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;./examples/pytorch/gpt/utils/megatron_ckpt_convert.py&#34;</span>, line 646, in main
</span></span><span class="line"><span class="cl">    convert_checkpoint<span class="o">(</span>args<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;./examples/pytorch/gpt/utils/megatron_ckpt_convert.py&#34;</span>, line 552, in convert_checkpoint
</span></span><span class="line"><span class="cl">    <span class="nv">vocab</span> <span class="o">=</span> json.load<span class="o">(</span>vocab_file<span class="o">)</span>
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/lib/python3.7/json/__init__.py&#34;</span>, line 293, in load
</span></span><span class="line"><span class="cl">    <span class="k">return</span> loads<span class="o">(</span>fp.read<span class="o">()</span>,
</span></span><span class="line"><span class="cl">  File <span class="s2">&#34;/usr/lib/python3.7/codecs.py&#34;</span>, line 322, in decode
</span></span><span class="line"><span class="cl">    <span class="o">(</span>result, consumed<span class="o">)</span> <span class="o">=</span> self._buffer_decode<span class="o">(</span>data, self.errors, final<span class="o">)</span>
</span></span><span class="line"><span class="cl">UnicodeDecodeError: <span class="s1">&#39;utf-8&#39;</span> codec can<span class="err">&#39;</span>t decode byte 0x80 in position 10702: invalid start byte
</span></span></code></pre></div><p>sol:</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="k">if</span> args.vocab_path:
</span></span><span class="line"><span class="cl">        <span class="nv">vocab_path</span> <span class="o">=</span> pathlib.Path<span class="o">(</span>args.vocab_path<span class="o">)</span>
</span></span><span class="line"><span class="cl">        with vocab_path.open<span class="o">(</span><span class="s2">&#34;rb&#34;</span><span class="o">)</span> as vocab_file:
</span></span><span class="line"><span class="cl">            <span class="nv">vocab</span> <span class="o">=</span> json.load<span class="o">(</span>vocab_file<span class="o">)</span>
</span></span><span class="line"><span class="cl">        start_id, <span class="nv">end_id</span> <span class="o">=</span> vocab<span class="o">[</span>DEFAULT_START_TAG<span class="o">]</span>, vocab<span class="o">[</span>DEFAULT_END_TAG<span class="o">]</span>
</span></span><span class="line"><span class="cl">    <span class="k">else</span>:
</span></span><span class="line"><span class="cl">        <span class="c1"># hard coded values from english gpt_vocab.json file</span>
</span></span><span class="line"><span class="cl">        start_id, <span class="nv">end_id</span> <span class="o">=</span> str<span class="o">(</span>OPENAI_GPT2_START_ID<span class="o">)</span>, str<span class="o">(</span>OPENAI_GPT2_END_ID<span class="o">)</span>
</span></span><span class="line"><span class="cl">改成
</span></span><span class="line"><span class="cl">start_id, <span class="nv">end_id</span> <span class="o">=</span> 0,2
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">解释
</span></span><span class="line"><span class="cl">start_id, end_id是dynamic decode阶段用的，是sampling阶段。
</span></span><span class="line"><span class="cl">start id没有用，end id是策略指定的一个特殊字符
</span></span></code></pre></div><p>转化成功：</p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2a56def6-e8d7-4a80-9e35-98141c063b3d/Untitled.png"
        alt="Untitled"/></p>
<p>TODO dynamic decoding layer做了什么</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">rm -rf build
</span></span><span class="line"><span class="cl">mkdir build
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> build
</span></span><span class="line"><span class="cl">cmake -DSM<span class="o">=</span><span class="m">80</span> -DCMAKE_BUILD_TYPE<span class="o">=</span>Release -DBUILD_MULTI_GPU<span class="o">=</span>OFF ..
</span></span><span class="line"><span class="cl">make -j256
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># 检查一下配置</span>
</span></span><span class="line"><span class="cl">vim ../examples/cpp/gpt/gpt_config.ini
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">FT_DEBUG_LEVEL</span><span class="o">=</span>DEBUG
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">FT_LOG_LEVEL</span><span class="o">=</span>DEBUG
</span></span><span class="line"><span class="cl"><span class="c1"># 选一个空卡</span>
</span></span><span class="line"><span class="cl"><span class="nb">export</span> <span class="nv">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="m">4</span>
</span></span><span class="line"><span class="cl">./bin/gpt_example &gt; log
</span></span></code></pre></div><p>但是还是在加载key_value的weight的时候遇到了困难</p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f4e3933d-959a-4970-aab1-1097845c9761/Untitled.png"
        alt="Untitled"/></p>
<p>cudaD2Dcpy有报错，</p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/5456df89-c9d6-4ced-becf-d3729253b14c/Untitled.png"
        alt="Untitled"/></p>
<p>原来是我把*写成,了</p>
<p>但是立刻有了新的报错，</p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/6365f944-877d-40f9-9f34-62ea346a5698/Untitled.png"
        alt="Untitled"/></p>
<p>然后发现策略的ckpt有问题，重新保存之后再看，发现是head num &amp; inter size配置错了</p>
<ul>
<li>load词表有nullptr，发现是构造函数列表用了=0的成员变量</li>
<li>打印出来bias是0，发现是权重是data_type=fp32，但是ini配置是data_type=fp16，于是改了ini再看，bias不是0了</li>
<li></li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl"><span class="c1"># 更新代码 outside docker</span>
</span></span><span class="line"><span class="cl">rm -rf src
</span></span><span class="line"><span class="cl">rm 1.tar
</span></span><span class="line"><span class="cl">y
</span></span><span class="line"><span class="cl">wget bjhw-www-t4-001.bjhw.baidu.com:8761/1.tar
</span></span><span class="line"><span class="cl">tar -xvf 1.tar
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1"># in docker build dir</span>
</span></span><span class="line"><span class="cl">make -j1024
</span></span><span class="line"><span class="cl"><span class="nb">unset</span> FT_DEBUG_LEVEL
</span></span><span class="line"><span class="cl"><span class="nb">unset</span> FT_LOG_LEVEL
</span></span><span class="line"><span class="cl">./bin/gpt_example  &gt; log.fp32.full
</span></span><span class="line"><span class="cl">grep  -A <span class="m">1</span> <span class="s2">&#34;debug query_weight.kerne&#34;</span> log.new.fp32
</span></span><span class="line"><span class="cl">grep  -A <span class="m">1</span> <span class="s2">&#34;debug query_weight.bias&#34;</span> log.new.fp32
</span></span><span class="line"><span class="cl">grep  -A <span class="m">1</span> <span class="s2">&#34;debug attention_input&#34;</span> log.new.fp32
</span></span><span class="line"><span class="cl">grep  -A <span class="m">1</span> <span class="s2">&#34;debug qkv_buf_&#34;</span> log.new.fp32
</span></span><span class="line"><span class="cl">grep  -A <span class="m">1</span> <span class="s2">&#34;debug qkv_buf_4_&#34;</span> log.new.fp32
</span></span><span class="line"><span class="cl">grep  -A <span class="m">1</span> <span class="s2">&#34;debug q_buf_2&#34;</span> log.new.fp32
</span></span><span class="line"><span class="cl">grep  -A <span class="m">1</span> <span class="s2">&#34;debug k_buf_2&#34;</span> log.new.fp32
</span></span><span class="line"><span class="cl">grep  -A <span class="m">1</span> <span class="s2">&#34;debug v_buf_2&#34;</span> log.new.fp32
</span></span></code></pre></div><p>可恶，新的模型竟然bias是没有的，还要做新的特性兼容</p>
<p>每次新迭代都有新的特性要兼容</p>
<p>File &ldquo;/target/hwj_test/llama2/pretrain_tokenizer/huggingface_hub/hf_api.py&rdquo;, line 29, in <module>
from typing import (
ImportError: cannot import name &lsquo;Literal&rsquo; from &rsquo;typing&rsquo; (/usr/lib/python3.7/typing.py)</p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/37a9a117-2024-4e55-8949-c73033a58caf/Untitled.png"
        alt="Untitled"/></p>
<p>修改方法：</p>
<p><code>from typing_extensions import Literal</code></p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/47aa9e70-e0d6-483e-99d9-4e0470af7e72/Untitled.png"
        alt="Untitled"/></p>
<p>记得不用的weight要device free</p>
<p>晚上杨哥一起算显存占用13B模型</p>
<p>13<em>1000000</em>4/1024/1024</p>
<p>convert &amp; split,</p>
<p>pp=8 &amp; tp=1, using split_and_convert_process not merge and convert!</p>
<p>314572800/40/128/4/3/40/128=1</p>
<p>训练代码：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">http://hmp.baidu.com/#/baichuanTask?id<span class="o">=</span>job-0bb64c0dd2aaab63
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root/paddlejob/v2/llm_sft/demo
</span></span><span class="line"><span class="cl"><span class="nb">source</span> env.sh
</span></span><span class="line"><span class="cl">nohup python pred.fix.py querys/dqa_generation_prediction.txt <span class="m">7</span> ../../webgpt/jb.ep4.adam/ &gt; res/825.res <span class="p">&amp;</span>
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">如果要打印详细的debug信息，去在源码中改动（替换一下，用完改回来）
</span></span><span class="line"><span class="cl">/root/paddlejob/workspace/env_run/pkgs/py37/lib/python3.7/site-packages/transformers/models/llama/modeling_llama.py
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="nb">cd</span> /root/paddlejob/workspace/env_run/pkgs/py37/lib/python3.7/site-packages/transformers/models/llama/
</span></span></code></pre></div><p>docker container 卡住了咋办</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-bash" data-lang="bash"><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"><span class="c1">#Get the container id</span>
</span></span><span class="line"><span class="cl">docker ps
</span></span><span class="line"><span class="cl"><span class="c1">#Get the containerd-shim process id</span>
</span></span><span class="line"><span class="cl">sudo ps awx <span class="p">|</span> grep containerd-shim <span class="p">|</span> grep <span class="s">&lt;&lt;container_id&gt;&gt; | awk &#39;{print $1}&#39;
</span></span></span><span class="line"><span class="cl"><span class="s">kill the process id
</span></span></span><span class="line"><span class="cl"><span class="s">sudo kill -9 &lt;&lt;process_id&gt;&gt;
</span></span></span><span class="line"><span class="cl"><span class="s">
</span></span></span><span class="line"><span class="cl"><span class="s">nvidia-docker run -it --shm-size 200G --name hwj_test  --network host -v /home/disk1/huwanjing/trt2022_src:/target   registry.baidubce.com/paddlepaddle/paddle:2.4.2-gpu-cuda11.2-cudnn8.2-trt8.0
</span></span></span><span class="line"><span class="cl"><span class="s">
</span></span></span><span class="line"><span class="cl"><span class="s">sudo ps awx | grep container</span>d-shim <span class="p">|</span> grep b0bdc85b0b0b
</span></span><span class="line"><span class="cl">docker container stop b0bdc85b0b0b
</span></span><span class="line"><span class="cl">docker container rm b0bdc85b0b0b
</span></span></code></pre></div><p>activation全0，可恶</p>
<p>bias强制nullptr看看</p>
<ol>
<li>gated做了Silu而不是up做的silu，diff1</li>
<li>__expf精度丢失？换成exp看看</li>
<li>用python算一下这俩数字看看是不是一样的</li>
</ol>
<p>mxps运行</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-markdown" data-lang="markdown"><span class="line"><span class="cl">export NCCL_LAUNCH_MODE=GROUP
</span></span><span class="line"><span class="cl">export CUDA_VISIBLE_DEVICES=5
</span></span><span class="line"><span class="cl">./bin/mxps
</span></span></code></pre></div><p>input token len = 3000 bs=1</p>
<p>mem 30592MB</p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/2bebfae4-183c-40ce-bb65-153c7f0a50a8/Untitled.png"
        alt="Untitled"/></p>
<p>bs=10</p>
<p>mem 56024MiB</p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/21b0e1c1-3d71-4307-9f33-ab86751948f1/Untitled.png"
        alt="Untitled"/></p>
<p>bs=16</p>
<p>mem 72896MiB / 81251MiB</p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/f322de6d-b72d-4beb-8525-1bd818048169/Untitled.png"
        alt="Untitled"/></p>
<p>bs=18</p>
<p>mem 78528MiB</p>
<p><img  src="https://s3-us-west-2.amazonaws.com/secure.notion-static.com/c54e88bc-c875-4ecb-ab7b-5040b941e5b7/Untitled.png"
        alt="Untitled"/></p>
<p>极限按照bs=18</p>
<p>bs=20，OOM</p>
<p>学习一些dream school的pdf</p>
<p><a href="https://blog.enmingw32.dev/SITCON2021/sitcon.pdf"target="_blank" rel="noopener noreferrer">https://blog.enmingw32.dev/SITCON2021/sitcon.pdf</a></p>
<p>问题</p>
<ol>
<li>什么数据在shred memory中？什么数据可以放在block中执行？（因为block执行没有先后顺序，也没有办法sync），不同thread的执行可以sync，那么什么数据在thread中运算？在同一个block下，每32thread是一个warp。
<ol>
<li>cuda并行设计思路：先看哪些数据只能串行执行，对最小串行单元进行并行计算的划分。并行thread-block纬度，</li>
</ol>
</li>
</ol>
<p><img  src="https://prod-files-secure.s3.us-west-2.amazonaws.com/df9619d4-48ca-42c4-bba7-db9921b1d030/94cf780f-ba80-4150-879b-a519ddef6afe/Untitled.png"
        alt="Untitled"/></p>
<p>mpi头文件报错</p>
<p><a href="https://stackoverflow.com/questions/26920083/fatal-error-mpi-h-no-such-file-or-directory-include-mpi-h"target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/26920083/fatal-error-mpi-h-no-such-file-or-directory-include-mpi-h</a></p>
<ol>
<li>安装open mpi 我选择了最新版本，希望依赖兼容</li>
</ol>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-markdown" data-lang="markdown"><span class="line"><span class="cl">解压压缩后：
</span></span><span class="line"><span class="cl">mkdir build
</span></span><span class="line"><span class="cl">cd build
</span></span><span class="line"><span class="cl">../configure --prefix=/target/openmpi-4.1.5/build/orz 2&gt;<span class="err">&amp;</span>1 | tee config.out
</span></span><span class="line"><span class="cl">make -j1024 all install
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">shell$ gunzip -c openmpi-4.1.5.tar.gz | tar xf -
</span></span><span class="line"><span class="cl">shell$ cd openmpi-4.1.5
</span></span><span class="line"><span class="cl">shell$ ./configure --prefix=/usr/local
</span></span><span class="line"><span class="cl"><span class="p">&lt;</span><span class="nt">...lots</span> <span class="na">of</span> <span class="na">output</span><span class="err">...</span><span class="p">&gt;</span>
</span></span><span class="line"><span class="cl">shell$ make all install
</span></span></code></pre></div><p>还没有结束！</p>
<p><img  src="https://prod-files-secure.s3.us-west-2.amazonaws.com/df9619d4-48ca-42c4-bba7-db9921b1d030/bb8f9024-d968-4502-968f-bd5ff2b7af0b/Untitled.png"
        alt="Untitled"/></p>
<p>还有一些lib没找着</p>
<p>我知道了 make的时候要</p>
<p>cmake -DSM=80 -DDMPI_INCLUDE_PATH=/usr/local/openmpi  -DCMAKE_C_COMPILER=gcc -DCMAKE_BUILD_TYPE=Release -DBUILD_MULTI_GPU=ON ..</p>
<p>cmake -DSM=80 -DCMAKE_BUILD_TYPE=Release -DBUILD_MULTI_GPU=ON -DNCCL_LIBRARIES=/usr/local/openmpi ..</p>
<p>-DMPI_CXX_LIB_NAMES= -DMPI_CXX_WORKS</p>
<p>MPI_CXX_FOUND</p>
<p>Could NOT find MPI_CXX (missing: MPI_CXX_LIB_NAMES MPI_CXX_WORKS)</p>
<h1 id="202389">2023.8.9</h1>
<p>今天是立秋；想到了21年那个立秋。</p>
<p>docker好神奇哦，可以看到同样在一个docker的人在做什么</p>
<p>配置converter环境小记</p>
<ul>
<li>torch是cpu版本的，遇到的gpu同步功能和接口没有办法使用</li>
<li>dynamic decoder的startid没用，endid是策略自定义，在无法从json文件中load vocab的时候手动赋值endid</li>
<li></li>
</ul>
<p>人类如何理解语言…</p>
</article>
</div>
<div class="article bottom"><section class="article navigation"><p><a class="link" href="/post/%E5%8C%97%E4%BA%AC%E7%9A%84%E4%B8%80%E4%B8%AA%E9%9B%A8%E5%A4%A9/"><span class="iconfont icon-article"></span>北京的一个雨天</a></p><p><a class="link" href="/post/2023.8.5/"><span class="iconfont icon-article"></span>周末小记</a></p></section></div></section><section id="footer"><div class="footer-wrap">
    <p class="copyright">©2019 Notepadium.</p>
    <p class="powerby"><span>Powered&nbsp;by&nbsp;</span><a href="https://gohugo.io" 
        target="_blank" rel="noopener noreferrer">Hugo</a><span>&nbsp;&amp;&nbsp;</span><a href="https://themes.gohugo.io/hugo-notepadium/" 
        target="_blank" rel="noopener noreferrer">Notepadium</a>
<a href='https://ipv6-test.com/validate.php?url=referer'
  ><img src='https://ipv6-test.com/button-ipv6-80x15.png' 
        alt='ipv6 ready' title='ipv6 ready' border='0' 
/></a>
</p></div></section>
  

</body>

</html>